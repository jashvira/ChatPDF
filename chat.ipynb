{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "import re\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_document(\n",
    "        file_path: str, overlap: int = 0) -> Tuple[List[Dict[str, str]], Dict[int, str]]:\n",
    "    \"\"\"Load a document from a file.\"\"\"\n",
    "    doc = []\n",
    "    para_dict = {}\n",
    "    resource_manager = PDFResourceManager()\n",
    "    fake_file_handle = io.StringIO()\n",
    "    converter = TextConverter(\n",
    "        resource_manager,\n",
    "        fake_file_handle,\n",
    "        laparams=LAParams())\n",
    "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "\n",
    "    para_id = 0\n",
    "    prev_para = None\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_document = PDFDocument(PDFParser(file))\n",
    "        for page_number, page in enumerate(\n",
    "                PDFPage.create_pages(pdf_document), start=1):\n",
    "            page_interpreter.process_page(page)\n",
    "            text = fake_file_handle.getvalue()\n",
    "\n",
    "            paragraphs = re.split(r'(?<!\\d)\\n\\n|(?<=[a-zA-Z])\\n\\n', text)\n",
    "            for paragraph in paragraphs:\n",
    "\n",
    "                paragraph = re.sub(r\"-\\n|\\n\", \"\", paragraph)\n",
    "\n",
    "                if re.search(\n",
    "                    r'\\d$|\\d \\w+$|\\d (\\w+ ){1,2}\\w+$|^\\w+$',\n",
    "                        paragraph):\n",
    "                    prev_para = paragraph\n",
    "                    continue\n",
    "\n",
    "                if prev_para:\n",
    "                    doc.append({\"page_content\": prev_para, \"metadata\": {\n",
    "                                \"source\": file_path, \"page\": page_number, \"para\": para_id}})\n",
    "                    prev_para = None\n",
    "\n",
    "                if paragraph.strip() != '' and bool(\n",
    "                        re.search('[a-zA-Z0-9]', paragraph)):\n",
    "\n",
    "                    para_dict[para_id] = paragraph\n",
    "\n",
    "                    sentences = sent_tokenize(paragraph)\n",
    "                    for i in range(len(sentences)):\n",
    "\n",
    "                        window_sentences = sentences[i:i + overlap + 1]\n",
    "\n",
    "                        sentence = ' '.join(window_sentences)\n",
    "\n",
    "                        sentence = re.sub(r\"-\\n|\\n\", \"\", sentence)\n",
    "                        if sentence.strip() != '' and bool(\n",
    "                                re.search('[a-zA-Z]', sentence)):\n",
    "                            doc.append({\"page_content\": sentence, \"metadata\": {\n",
    "                                       \"source\": file_path, \"page\": page_number, \"para\": para_id}})\n",
    "                    para_id += 1\n",
    "\n",
    "            fake_file_handle.truncate(0)\n",
    "            fake_file_handle.seek(0)\n",
    "\n",
    "    if prev_para:\n",
    "        doc.append({\"page_content\": prev_para, \"metadata\": {\n",
    "                    \"source\": file_path, \"page\": page_number, \"para\": para_id - 1}})\n",
    "\n",
    "    converter.close()\n",
    "    fake_file_handle.close()\n",
    "\n",
    "    return doc, para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, para_dict = load_pdf_document(\n",
    "    '/path', overlap=3)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "model = INSTRUCTOR('hkunlp/instructor-xl')\n",
    "\n",
    "sentence_pairs = []\n",
    "for chunk in documents:\n",
    "    sentence = chunk[\"page_content\"]\n",
    "    instruction = \"Represent the Research paper document chunk for retrieval:\"\n",
    "    sentence_pairs.append([instruction, sentence])\n",
    "\n",
    "embeddings = model.encode(\n",
    "    sentence_pairs,\n",
    "    batch_size=20,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True)\n",
    "\n",
    "for chunk, embedding in zip(documents, embeddings):\n",
    "    chunk[\"embeddings\"] = np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=50, n_components=10, random_state=4)\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=3, gen_min_span_tree=True)\n",
    "\n",
    "\n",
    "clusterer.fit(embeddings_2d)\n",
    "\n",
    "\n",
    "cluster_labels = clusterer.labels_\n",
    "\n",
    "\n",
    "for chunk, cluster_id in zip(documents, cluster_labels):\n",
    "    chunk[\"metadata\"][\"cluster\"] = cluster_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x, y):\n",
    "    return (np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))).item()\n",
    "\n",
    "\n",
    "def check_no_of_tokens(prompt):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    return len(encoding.encode(prompt))\n",
    "\n",
    "\n",
    "def create_prompt(query, top_chunks_with_context):\n",
    "    prompt = \"Your task is to answer questions based on a research paper.\" + \"\\n\\n\"\n",
    "    prompt += \"Answer the following question: \" + query + \"\\n\\n\" + \"Below is the only information you have about the paper, wade through the irrelevant text and use the useful text. Do not expect extremely precise information.\" + \\\n",
    "        \"\\n\\n\" + \"\\n\\n\".join(chunk for chunk in top_chunks_with_context)\n",
    "    prompt += \"\\n\\n\" + \"Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "openai.api_key = \"KEY\"\n",
    "TOPK = 20\n",
    "PROMPT_LENGTH = 800\n",
    "\n",
    "\n",
    "instruction = \"Represent the Research paper question for retrieving supporting document:\"\n",
    "query = \" How to explain the phenomenon observed in this paper?\"\n",
    "query_embedding = model.encode([[instruction, query]])\n",
    "\n",
    "similarities = [\n",
    "    (chunk, cosine(query_embedding, chunk[\"embeddings\"].T))\n",
    "    for _, chunk in enumerate(documents)]\n",
    "\n",
    "top_chunks = sorted(similarities, key=lambda x: x[1], reverse=True)[:TOPK]\n",
    "top2_cluster_ids = [chunk[0][\"metadata\"][\"cluster\"] for chunk in top_chunks[:2]]\n",
    "filtered_top_chunks = [\n",
    "    chunk for chunk in top_chunks\n",
    "    if chunk[0][\"metadata\"][\"cluster\"] in top2_cluster_ids]\n",
    "\n",
    "\n",
    "top_para_ids = list(set([chunk[0][\"metadata\"][\"para\"]\n",
    "                    for chunk in filtered_top_chunks]))\n",
    "top_chunks_with_context = [para_dict[para_id] for para_id in top_para_ids]\n",
    "\n",
    "prompt = create_prompt(query, top_chunks_with_context)\n",
    "while check_no_of_tokens(prompt) > PROMPT_LENGTH:\n",
    "    top_chunks_with_context = top_chunks_with_context[:-1]\n",
    "    prompt = create_prompt(query, top_chunks_with_context)\n",
    "\n",
    "print(\"Number of tokens used in prompt: \", check_no_of_tokens(prompt))\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
